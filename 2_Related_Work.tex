\chapter{Related Work}
\label{Related_Work}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Predicting Hypoxia from FDG-PET and CT}
\label{Related_Work-hypoxia_prediction}
In a hypoxia PET image, the tumor is the sole object of interest whose exact location and shape is usually known before the hypoxia image acquisition. Zegers et al. \cite{zegers2014vivo} show the presence of significant correlation among tumor-level parameters -- tumor size, overall tumor metabolism (indicated by FDG-PET uptake) and overall tumor hypoxia (indicated by HX4-PET uptake). They additionally perform sub-volume level (i.e. voxel-wise) analysis between FDG-PET and HX4-PET to evaluate the spatial similarity of both tracers' uptake patterns within the tumor, and observe a reasonable correlation between them. However, they also note that this correlation is not significant likely due to the involvement of genetic properties of the tumor, and conclude that HX4-PET imaging does indeed provide information complementary to FDG-PET. Even et al. \cite{even2017predicting} investigate the possibility of predicting hypoxia patterns in the tumor using other indirect markers of hypoxia including anatomy (CT), metabolism (FDG-PET), and blood perfusion parameters (tumor blood flow and blood volume maps obtained from Dynamic CT). They argue that a voxel-wise regression between the inputs and the HX4-PET ground truth would be negatively affected by registration-related imperfections, and instead use a supervoxel based approach for robustness. They train random-forest-based regression models on simple features -- median, standard deviation and entropy -- derived from the supervoxels in each input image modality to infer the level of hypoxia in the corresponding supervoxel regions. The models were shown to reliably predict the spatial distribution of tumor hypoxia, and the features from CT and FDG-PET were found to be the most informative. Sanduleanu et al. \cite{sanduleanu2020non} conduct a large-scale radiomic study using data from six different medical centers. Radiomic features comprise a large set of standardized quantitative features extracted from medical images developed with the aim of supporting high-throughput automated image analysis in radiation oncology \cite{aerts2014decoding}. The authors hypothesize that the combined radiomic features derived from both CT and FDG-PET modalities can predict tumor hypoxia status more effectively compared to either of these modalities alone, and build and validate multiple random-forest-based tumor classification models. They conclude with the finding that the radiomic models using both modalities did indeed classify the tumors as hypoxic or non-hypoxic with high accuracy.

In this work, we utilize CT and FDG-PET information and aim at predicting hypoxia patterns in the entire body region covered by the input images, instead of focusing on just the tumor locality. In order to synthesize full HX4-PET images accurately, the HX4 tracer uptake in the human body must be effectively modeled as a function of CT-derived anatomical features and FDG tracer uptake. Then, inferring the HX4 uptake within the tumor becomes a special case of inferring it at any given region in the body, and this special case is characterized by the presence of abnormal structures in CT and an immense amount of activity in FDG-PET.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Image-to-Image Translation GANs}
Generative adversarial networks (GANs) \cite{goodfellow2014generative} are a class of deep generative models comprised of two networks -- the generator and the discriminator -- that are jointly trained in a competitive setting. The generator's task is to synthesize samples (often images) that are indistinguishable from samples from the true data distribution, whereas the task of the discriminator is to accurately distinguish between real and synthetic samples. Over the training period, the generator learns to produce increasingly realistic samples in response to the discriminator's feedback, which itself also improves. The model eventually converges to an equilibrium state where the generator ideally models the real data distribution, and as a result, the best the discriminator can do is random guessing. While the user has no control over the semantics of the images generated by the GAN model, the development of Conditional GAN (cGAN) \cite{mirza2014conditional} has allowed using simple semantic information, such as class labels, to train a conditional generative model. During inference, this enables the user to query the generator to synthesize images belonging to a given class. 

Based on cGAN, Isola et al. \cite{isola2017image} introduce the Pix2Pix system as a general-purpose framework for solving image-to-image translation problems. Any computer vision or graphics task that takes as input an image and expects another image as its output having the same size and containing the same semantic information can be formulated as image-to-image translation. For example, semantic segmentation of photographs, synthesis of photographs from semantic label map or edge map, and colorization of monochrome photos. In such problems, the input and the output images are viewed as different renderings of the same underlying scene. The Pix2Pix generator, conditioned on the entire input image, produces an output image that is directly compared to the ground truth via a pixel-wise L1 loss. Additionally, as a consequence of the adversarial training, the Pix2Pix discriminator model serves as a ``structured" loss function that is learned from the data itself. This essentially rules out the need to handcraft custom loss functions for each image translation task. Pix2Pix, however, requires pixel-wise aligned input-target image pairs, i.e. \textit{paired} data, for training. A classic example task where paired data doesn't exist is horse-to-zebra translation \cite{zhu2017unpaired}. Based on ideas similar to Pix2Pix, Zhu et al. present the CycleGAN system \cite{zhu2017unpaired} for learning an image translation model from unpaired data. CycleGAN consists of four deep neural networks, which compose two generator-discriminator pairs. Given two sets of images, each containing images of one \textit{domain}, the CycleGAN model learns a forward mapping between the input and the target domains as well as its inverse. This pair of mappings is used to implement a cycle-consistency mechanism to encourage the preservation of high-level structure (i.e. the ``content") across an input image and its translated counterpart in the target domain. Since there is no direct way to ensure content preservation between input and output images due to the absence of paired data, this strategy serves as an indirect means to achieve this.

The Pix2Pix and CycleGAN systems have been widely applied in medical imaging research to various cross-modality image translation use cases \cite{yi2019generative} with slight custom modifications in network architecture and loss functions, although we limit our discussion to works that focus on synthesizing contrast maps from CT images. Chandrashekar et al. \cite{chandrashekar2020deep} show that sufficient contrast exists between blood and soft tissue to differentiate them in CT angiograms acquired without a contrast agent, and train a 2D CycleGAN model to selectively enhance the existing contrast and accurately simulate iodine-based contrast effect in the CT images. Haubold et al. \cite{haubold2021contrast} instead investigate into reducing the amount of required iodine-based contrast agent dose from full dose to as low as 50\%, and apply a 2D Pix2Pix-based post-processing step to provide the remaining contrast to the CT. Motivated by the drawback of multimodal FDG-PET/CT-based lesion detection systems requiring FDG-PET acquisition, Ben-Cohen et al. \cite{bencohen2018crossmodality} investigate synthesizing FDG-PET images from just CT scans. The authors use a two-step approach combining a fully convolutional regression network with a Pix2Pix GAN. They argue that CT-derived synthetic FDG-PET can potentially substitute a clinically acquired FDG-PET scan for improving tumor visibility, although they do not provide any physiological basis for image translation problem. Bi et al. \cite{bi2017synthesis}, as opposed to other studies which focus on the direct clinical application of synthetic images, aim at synthesizing realistic FDG-PET samples for providing data augmentation to train auxiliary deep-learning-based systems, for example, malignancy detection models. In order to preserve the position and shape of the tumor across translation, they utilize explicitly the tumor annotation together with the CT image in a two-channel Pix2Pix model to generate the corresponding FDG-PET. 

Based on our literature survey, we found no prior work that investigated cross-modality translation for synthesizing hypoxia PET from other modalities. We explore both the paired and unpaired image translation approaches, using Pix2Pix and CycleGAN systems, respectively, since they are among the most straightforward and widely applied image translation methods.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation of Synthetic Medical Images}

Evaluation of GAN-translated synthetic images is a difficult problem even when the ground truth reference images are available. In quantitative medical images, pixel-wise differences such as mean-squared error (MSE) and peak signal-to-noise ratio (PSNR) can be useful in only partially assessing image quality since they do not account for the image statistics and local structure. Perceptual metrics like structural similarity index (SSIM) have, therefore, been widely applied to assess the synthetic images based on the similarity of their perceived local structure with that of the ground truth \cite{yi2019generative}. Image registration literature provides entropy-based metrics for image matching, such as normalized mutual information (NMI) \cite{studholme1999overlap}, that can measure structural alignment even across different image modalities. Mutual information has been applied as an image quality metric in works concerning translation across Magnetic Resonance (MR) image sequences \cite{yang2018mri, welander2018generative}. 

In many cases, the end goal of the synthetic images is to potentially substitute the clinically acquired scans, and therefore, image evaluation must be application-specific and clinically relevant. It is, therefore, common to perform downstream tasks using synthetic images to measure their clinical value. Haubold et al. \cite{haubold2021contrast} include in their evaluation a manual ``pathological consistency" testing procedure where synthetic image slices are compared to their corresponding ground truth slices by trained radiologists to check whether the synthetic image preserves the patient's existing pathology and doesn't insert pathology that is non-existent in the ground truth. Ben-Cohen et al. \cite{bencohen2018crossmodality} evaluate their synthetic FDG-PET images by using them, in combination with CT, to detect lesions in the image and measuring the detection performance.  

We perform an evaluation on our synthetic HX4-PET images via both general image quality metrics and application-specific downstream image analysis. Because each image quality metric has its own strengths and weaknesses, we use a multitude of them to effectively quantify the fidelity of the synthetic images. Additionally, a systematic visual inspection is performed to identify common failure modes of the image translation GANs used. As downstream tasks, we perform binary classification of the tumors as hypoxic or non-hypoxic and segmentation of the 3D hypoxic regions within the tumor. HX4-PET images are quantitative in nature and have established procedures for hypoxia quantification \cite{zegers2013hypoxia}. The hypoxia quantification tasks are, therefore, performed using simple thresholding-based methods with standard threshold values derived from the relevant clinical literature \cite{zegers2013hypoxia, even2017predicting}.