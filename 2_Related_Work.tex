\chapter{Related Work}
\label{Related_Work}

\section{Predicting Hypoxia from FDG-PET and CT}
\label{Related_Work-hypoxia_prediction}
In a hypoxia PET image, the tumor is the sole object of interest whose exact location and shape is usually known before the hypoxia image acquisition. Zegers et al. \cite{zegers2014vivo} show the presence of significant correlation among tumor-level parameters -- tumor size, overall tumor metabolism (indicated by FDG-PET uptake) and overall tumor hypoxia (indicated by HX4-PET uptake). They additionally perform sub-volume level (i.e. voxel-wise) analysis between FDG-PET and HX4-PET to evaluate the spatial similarity of both tracers' uptake patterns within the tumor, and observe a reasonable correlation between them. However, they also note that this correlation is not significant due to the involvement of genetic properties of the tumor, and conclude that HX4-PET imaging does indeed provide information complementary to FDG-PET. Even et al. \cite{even2017predicting} investigate the possibility of predicting hypoxia patterns in the tumor using other indirect markers of hypoxia including anatomy (CT), metabolism (FDG-PET) and blood perfusion parameters (tumor blood flow and blood volume maps obtained from Dynamic CT). They argue that a voxel-wise regression between these inputs and the HX4-PET ground truth would be affected by registration-related imperfections, and instead use a supervoxel based approach for robustness. They train random-forest-based regression models on simple features -- median, standard deviation and entropy -- derived from the supervoxels in each input image modality to infer the level of hypoxia in the corresponding supervoxel regions. The models were shown to reliably predict the spatial distribution of tumor hypoxia, and the features from CT and FDG-PET were found to be most informative. Sanduleanu et al. \cite{sanduleanu2020non} conduct a large-scale radiomic study using data from six different medical centers. Radiomic features comprise a large set of standardized quantitative features extracted from medical images developed with the aim of supporting high-throughput automated image analysis in radiation oncology \cite{aerts2014decoding}. The authors hypothesize that combined radiomic features derived from both CT and FDG-PET can predict tumor hypoxia status more effectively compared to either of these modalities alone, and build and validate multiple random forest based tumor classification models. They conclude with the finding that the radiomic models using both modalities accurately classified tumors as hypoxic or non-hypoxic.

In this work, we utilize CT and FDG-PET information and aim at predicting hypoxia patterns in the entire anatomical region covered by the input images, instead of focusing on just the tumor locality. In order to synthesize full HX4-PET images accurately, the HX4 tracer uptake in the human body must be effectively modeled as a function of CT-derived anatomical features and FDG tracer uptake. Then, inferring the HX4 uptake within the tumor becomes a special case of inferring it at any given region in the body, and this special case is characterized by the presence of abnormal structures in CT and a very high uptake observed in FDG-PET. \todo{Refine these last statements. Do they make sense? Or is there another reason why producing full HX4-PET is preferable?}



\section{Image-to-Image Translation GANs}
Generative adversarial networks (GANs) \cite{goodfellow2014generative} are a class of deep-learning-based generative models comprised of two networks -- the generator and the discriminator -- that are jointly trained in a competitive setting. The generator's task is to synthesize samples (usually, images) that resemble samples from the true data distribution, whereas the task of the discriminator is to accurately distinguish between real and synthetic samples. Over the training period, both networks facilitate each other's improvement, eventually converging to an equilibrium state where the generator models the real data distribution and as a result, the best the discriminator can do is random guessing. While the user has no control over the semantics of the images generated by the GAN model, the development of Conditional GAN (cGAN) \cite{mirza2014conditional} allowed using simple semantic information, such as class labels, to train a conditional generative model. During inference, this enables the user to query the generator to synthesize images belonging a given class. 

Isola et al. \cite{isola2017image} introduced the Pix2Pix system as an extension of cGAN to serve as a general purpose framework for solving image-to-image translation problems. Any computer vision or graphics task that takes as input an image and expects another image of the same size and semantics as its output can be formulated as image-to-image translation. For example, semantic segmentation, synthesis of photographs from semantic label-map or edge-map, and colorization of monochrome photos. Here, the input and the output images are viewed as different renderings of the same underlying scene. The Pix2Pix generator, conditioned on the entire input image, produces an output image that is compared to the ground-truth via a pixel-wise L1 loss. Additionally, as a consequence of the adversarial training, the Pix2Pix discriminator model serves as a ``structured" loss function that is learned from the data itself. This essentially rules out the need to handcraft custom loss functions for each image-to-image translation task. Pix2Pix, however, requires pixel-wise aligned input-target image pairs, i.e. \textit{paired} data, for training. A classic example task where paired data doesn't exist is horse-to-zebra translation \cite{zhu2017unpaired}. The CycleGAN system \cite{zhu2017unpaired} was developed as an extension of the Pix2Pix framework for learning from unpaired data. CycleGAN consists of four deep neural networks, which form two generator-discriminator pairs. Given two sets of images, each containing images of one type or ``domain", the CycleGAN model learns a forward mapping between the input and the target domains as well as its inverse. The two mappings are used to implement cycle-consistency mechanism to encourage the preservation of high-level structure (i.e. the ``content") across an input image and its translated counterpart in the other domain. Since there is no direct way to ensure content preservation between input and output images due to the absence of paired data, this strategy serves as an indirect means to achieve this.

The Pix2Pix and CycleGAN systems have been widely applied in medical imaging research to cross-modality image translation use-cases \cite{yi2019generative} with slight modifications in network architecture and loss functions, although, we focus only on the works related to synthesizing contrast maps from CT images. Chandrashekar et al. \cite{chandrashekar2020deep} show that sufficient contrast exists between blood and soft tissue to differentiate them in CT angiograms acquired without a contrast agent, and train a 2D CycleGAN model to selectively enhance the existing contrast and accurately simulate iodine-based contrast effect in the CT images. Haubold et al. \cite{haubold2021contrast} instead investigate on reducing the amount of required iodine-based contrast agent dose to 50\% and applying a 2D Pix2Pix-based post-processing step to provide the remaining contrast to the CT. Motivated by the drawback of multimodal FDG-PET/CT based lesion detection systems which require FDG-PET acquisition, Ben-Cohen et al. \cite{bencohen2018crossmodality} investigate on synthesizing FDG-PET images from just CT scans. The authors use a two-step approach combining a fully-convolutional regression network with a Pix2Pix GAN. They argue that CT-derived synthetic FDG-PET can potentially substitute a clinically acquired FDG-PET scan for improving tumor visibility, although they do not provide any physiological grounding for this claim. Bi et al. \cite{bi2017synthesis}, as opposed to other studies which focus on direct clinical application of the synthetic images, aim at synthesizing realistic FDG-PET images for providing data augmentation to train secondary deep-learning based systems, for example, malignancy detection models. They use a two-channel Pix2Pix model that takes tumor annotation along with the CT image to generate the corresponding FDG-PET in order to preserve the position and shape of the tumor across translation. 

Based on our literature survey, we found that no prior work has been done in the direction of GAN-based hypoxia PET synthesis. We investigate both paired and unpaired approaches, using Pix2Pix and CycleGAN systems as they are the most straightforward and widely applied image translation methods. We also note that none of the related works involved more than two imaging modalities, and in our unique use-case, applying the CycleGAN naively would not sensible. We, therefore, propose an alteration in the design to optimize it for our use-case, as discussed further in \ref{GAN_Systems}.



\section{Evaluation of Synthetic Medical Images}
% Basic image similarity metrics
% Application-specific analysis, esp if clinical application is the aim
% Data augmentation