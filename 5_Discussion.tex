\chapter{Discussion}
\label{Discussion}

% Research qs were answered
In experiments \ref{Expt_1} and \ref{Expt_2}, we observe that the paired Pix2Pix approach can generate higher quality images as compared to unpaired CycleGAN, although at the cost of acquiring and preparing the ground truth images. Among the two CycleGAN systems, CycleGAN-balanced was able to circumvent the non-invertibility issue of CycleGAN-naive due to its design modification and produced better results as indicated by both quantitative and qualitative analyses. Our quantitative evaluation of the synthetic HX4-PET images involved using a set of six metrics, of which three -- MSE, MAE, and PSNR -- measure the voxel-level accuracy of the synthetic images with respect to the ground truth, whereas the remaining three -- SSIM, NMI, and histogram distance -- account for the fidelity of image structure and statistics. Furthermore, each metric has its unique property and since the ``quality" of an image is a multifaceted attribute, using a population of such metrics for image assessment can capture the image quality more effectively as compared to using a subset of them. We thereby address our first research question. In \ref{cycelgan_convergence}, we perform an analysis of CycleGAN training losses over the training period and observe that the CycleGAN-balanced model was more stable and displayed better convergence properties as opposed to the CycleGAN-naive model, which diverged away from its objectives. The validation metrics collectively reflected similar trends as the convergence parameters while also being informative about model generalizability, thereby answering our third research question. Finally, through experiment \ref{Expt_3}, we address our second research question by identifying and performing clinically relevant downstream tasks to determine the value of our synthetic HX4-PET images. Tumor hypoxia measurement derived from synthetic images from Pix2Pix showed remarkably poor accuracy compared with the two CycleGANs. While a majority of the tumors in the synthetic images produced by the CycleGAN models were classified correctly, the classification rate in Pix2Pix predictions is lower. However, as indicated by a poor segmentation score, none of the models could predict accurately the spatial distribution of high hypoxia. The primary reason could be the lack of sufficient training data, and the second reason, specifically for Pix2Pix, could be the noise in its supervision signal caused due to imperfect ground truth images. We believe that given sufficient training data, unpaired models like CycleGAN, especially the modified version of it, could be more suitable for the HX4-PET synthesis task. As supplementary material, we make available the \textit{WandB} training reports for the depth-estimation task (experiment \ref{Expt_1}) \footnote{Training report for the depth-estimation task: \url{https://bit.ly/3x452Qi}} and the HX4-PET synthesis task (experiment \ref{Expt_2}) \footnote{Training report for the HX4-PET synthesis task: \url{https://bit.ly/35Y8dx2}} which include training losses, validation metrics and intermediate outputs of the models.

% Some limitations, and opportunities for future work (stuff to do after thesis):
%   1. Dataset size -- larger training set to train better models, larger validation set to test properly
%   2. CycleGANs appeared to focus more on FDG features. Would be nice to see through an ablation study whether or not they really use CT features. One way to do this is taking the same trained models, turning off the CT signal in their input and performing inference.
%   3. With metrics - Need to consult more theoretical literature to understand their mathematical properties. This would allow choosing better and more diverse metrics to use and evaluate with the voting method.
%   4. Turing test
There are several limitations of this thesis that can provide opportunities for future work. First, the small size of our Maastro Lung HX4-PET dataset was a serious limitation on model training and validation. A larger training dataset would allow training better translation models and a sufficiently large and diverse validation dataset would enable more effective evaluation of the models. Second, it was observed that both the CycleGAN models predicted tumor hypoxia patterns that closely resembled the FDG uptake signatures rather than the actual spatial distribution of hypoxia. The models might have learned to heavily rely on FDG-PET features while ignoring CT information. It would be interesting to verify this by performing an ablation study on these models. This could be conducted, for instance, by performing inference with the fully trained models with the CT signal set to zero value and observing the generator outputs. Third, the quality assessment of the synthetic HX4-PET images was performed using a set of six general-purpose image quality and similarity metrics that were logically chosen. However, this portfolio of evaluation metrics can be improved. It would be valuable to consult theoretical literature on image metrics and understand in a greater depth their mathematical properties. This would help in selecting more suitable and diverse metrics thereby improving the comprehensiveness of image assessment. Fourth, as an extension of the simple automated clinical evaluation performed in experiment \ref{Expt_3}, a version of the Turing test can be performed by presenting the synthetic HX4-PET images to a radiation oncologist, with a certain clinically relevant goal, for instance, determining the prognostic value of the synthetic images. 


% Fine tuning stuff (questionable. don't include)
% Second, the Pix2Pix model failed in accurately predicting tumor hypoxia and produced high-frequency noise patterns in the tumor region, although its synthetic HX4-PET images were of an overall high quality. In addition to the registration-related issues, this failure can also be attributed to model training. In addition to using a larger training dataset as a possible solution to reducing these artifacts, it would be interesting to investigate into \textit{fine tuning} the trained Pix2Pix model using the existing dataset. This could be performed, for instance, by first using a patch-sampling strategy that focuses more on the tumor vicinity. Then, the weights of the Pix2Pix generator's encoder (which is its feature extractor) could be frozen while updating only its decoder (which is its image synthesizer). The intuition behind this is that, in doing so, the noise patterns can potentially be reduced since they caused during the upsampling operations used the decoder path, and that this by learning better upsampling kernels


% Expt 3 stuff (need this? probably not)
% We argue that although none of our models' synthetic HX4-PET was clinically satisfactory, paired translation approaches such as Pix2Pix that assume voxel-wise alignment are bound to produce inaccuracies in their images at a scale of the tumor.
% In our dataset, HX4-PET scans for all patient were acquired on a different day from FDG-PET and pCT acquisition. A problem with tumor hypoxia is the fact that the hypoxic region has a tendency to change over time, even over a period of hours [cite ...]. Zegers et al. \cite{zegers2013hypoxia} raise this argument to explain the relatively low \textit{voxel-level} correlation they observed between FDG-PET and HX4-PET SUV values despite a significant correlation on \textit{tumor-level} properties between the two modalities. It is possible that in our data, the state of the tumor captured by FDG-PET/pCT might not correspond perfectly to its state captured by HX4-PET. In such a case, even if the HX4-PET image were to be perfectly registered over pCT and FDG-PET, a supervised method that assumes voxel-level correspondence would be less effective, to say the least. In this view, it could then be possible that the hypoxia patterns predicted by the CycleGANs, although not matching the ground truth, might reflect the ``actual" hypoxia distribution.